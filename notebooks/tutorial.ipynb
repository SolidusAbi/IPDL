{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "project_dir = os.path.split(os.getcwd())[0]\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from IPDL import MatrixEstimator, ClassificationInformationPlane, AutoEncoderInformationPlane\n",
    "from IPDL.optim import AligmentOptimizer, SilvermanOptimizer\n",
    "\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "from torch.nn.functional import one_hot\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook has the purpose to explain how to use this framework (IPDL). For any suggestion... meh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Design\n",
    "\n",
    "In order to obtain the Information Plane, it is necessary to generates the matrix $A_T$ which is a representation RKHS of the T layer's output. In this framework, this task is performed by the MatrixEstimator class which is necessary to indicate a intial $\\sigma$ value due to this framework apply RBF kernel in order to obtain the RKHS. \n",
    "\n",
    "The following cell is a example of network design where at the end of each layer we are applying a MatrixEstimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(1024, affine=True),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128, affine=True),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(64, affine=True),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(32, affine=True),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(32, 10),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weight_init(m)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def weight_init(self, module):\n",
    "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(module.weight.data, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In the training process, we have to define a optimizer for the *MatrixEstimator*, which is independent from the optimizer which is going to be used in order to optimize the network. This new optimizer, which base class is called *MatrixOptimizer*, will update the sigma value which is used in the RBF kernel.\n",
    "\n",
    "About the Information Plane, for this operation a specific class have been implemented, *InformationPlane*, which contains the *computeMutualInformation()* method which giving the input matrix, $A_x$, and output matrix $A_y$, is going to compute the mutual information $I(A_x,A_t)$ and $I(A_t,A_x)$ that are used for generate the Information Plane.\n",
    "\n",
    "For this example, MNIST dataset will be used..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformToTensor = Compose([ ToTensor() ])\n",
    "dataset = torchvision.datasets.MNIST(\"../datasets/MNIST/\", train=True, download=True, transform=transformToTensor)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [59488 , 512])\n",
    "train_dataloader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=0)\n",
    "eval_dataloader = DataLoader(val_set, batch_size=150, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to generate the matrices $A_x$ and $A_y$. In this case, the matrices are going to be generate directly but it could be generate applying the MatrixEstimator class. The sigma values used are the proposed in [referencia].."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPDL.functional import matrix_estimator\n",
    "\n",
    "val_inputs, val_targets = next(iter(eval_dataloader))\n",
    "val_inputs = val_inputs.flatten(1).to(device)\n",
    "val_targets = one_hot(val_targets, num_classes=10).float().to(device) \n",
    "    \n",
    "_, Ax = matrix_estimator(val_inputs, sigma=8)\n",
    "Ky, Ay = matrix_estimator(val_targets, sigma=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir nuestro modelo para crear el matrix optimizer y information plane..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "matrix_optimizer = AligmentOptimizer(model, beta=0.9, n_sigmas=200)\n",
    "ip = ClassificationInformationPlane(model, use_softmax=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.009, momentum=0.9)\n",
    "\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model(val_inputs)\n",
    "        \n",
    "    for inputs, labels in tqdm(train_dataloader):\n",
    "        inputs = inputs.flatten(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 50 == 0:\n",
    "            loss_record.append(running_loss / 50)\n",
    "            running_loss = 0.0\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model(val_inputs)\n",
    "            matrix_optimizer.step(Ky.to(device))\n",
    "            ip.computeMutualInformation(Ax.to(device), Ay.to(device))\n",
    "\n",
    "        if i > 500:\n",
    "            for ip in net.getInformationPlaneLayers():\n",
    "                ip.setNumberOfSigma(100)\n",
    "\n",
    "        i += 1\n",
    " \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPDL.utils import showMutualInformation\n",
    "showMutualInformation(ip, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "La optimización por *kernel aligment* no es posible para el caso de un autoencoder. En esos casos es más propio usar *Silverman’s rule of thumb*. Para kernels de alta dimensionalidad, propia de las vista en las redes neuronales, Nicolás I. Tapia et al. proponen una simplificación de este con una posible normalización teniendo en cuenta la dimensionalida. The *SilvermanOptimizer* is a implementation of this proposed method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(784, 512),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512, affine=False),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            # nn.Dropout(p=0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256, affine=False),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            # nn.Dropout(p=0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(128, affine=False),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            # nn.Dropout(p=0.1),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256, affine=False),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            # nn.Dropout(p=0.1),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512, affine=False),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "            # nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 784),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            nn.Sigmoid(),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weight_init(m)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def weight_init(self, module):\n",
    "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(module.weight.data, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformToTensor = Compose([\n",
    "            ToTensor(), # first, convert image to PyTorch tensor\n",
    "            Lambda(lambda x: torch.flatten(x)) # Auto-flatten\n",
    "        ])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\"../datasets/MNIST/\", train=True, download=True, transform=transformToTensor)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [59488 , 512])\n",
    "train_dataloader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=0)\n",
    "eval_dataloader = DataLoader(val_set, batch_size=512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from IPDL.functional import matrix_estimator\n",
    "\n",
    "val_inputs, val_targets = next(iter(eval_dataloader))\n",
    "val_inputs = val_inputs.flatten(1).to(device)\n",
    "\n",
    "n = val_inputs.size(0)\n",
    "d = val_inputs.size(1) if len(val_inputs.shape) == 2 else reduce(lambda x, y: x*y, val_inputs.shape[1:])\n",
    "gamma = 0.8\n",
    "sigma = gamma * n ** (-1 / (4+d)) * math.sqrt(d) \n",
    "\n",
    "_, Ax = matrix_estimator(val_inputs, sigma=sigma)\n",
    "Ax = Ax.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epoch = 25\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.1)\n",
    "matrix_optimizer = SilvermanOptimizer(model, gamma=0.8, normalize_dim=True)\n",
    "\n",
    "model.train()\n",
    "\n",
    "eval_inputs, _ = next(iter(eval_dataloader))\n",
    "eval_inputs = eval_inputs.to(device)\n",
    "\n",
    "epoch_iterator = tqdm(\n",
    "    range(n_epoch),\n",
    "    leave=True,\n",
    "    unit=\"epoch\",\n",
    "    postfix={\"lss\": \"%.6f\" % 0.0, \"vls\": \"%.6f\" % -1,},\n",
    ")\n",
    "\n",
    "ip = AutoEncoderInformationPlane(model)\n",
    "\n",
    "for epoch in epoch_iterator:\n",
    "    for idx, (inputs, _) in enumerate(train_dataloader):\n",
    "        inputs = inputs.flatten(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss_value = float(loss.item())        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 25 == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                outputs = model(eval_inputs.flatten(1))\n",
    "                eval_loss_value = float((criterion(outputs, eval_inputs.flatten(1))).item())\n",
    "                epoch_iterator.set_postfix(\n",
    "                    lss=\"%.6f\" % loss_value, vls=\"%.6f\" % eval_loss_value,\n",
    "                )\n",
    "\n",
    "            model.train()\n",
    "    \n",
    "    scheduler.step()\n",
    "    if epoch == 0: # Solo necesario una vez\n",
    "        matrix_optimizer.step()\n",
    "    \n",
    "    Ixt, Ity = ip.computeMutualInformation(Ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ixt)\n",
    "print(Ity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPDL import MatrixBasedRenyisEntropy\n",
    "with plt.style.context('seaborn'):\n",
    "    f, ax = plt.subplots(figsize=(12, 12))\n",
    "    reference = MatrixBasedRenyisEntropy.entropy(Ax).cpu()\n",
    "    ax.set(xlim=(0, reference), ylim=(0, reference))\n",
    "\n",
    "    Ixt, Ity = ip.getMutualInformation(moving_average_n=6)\n",
    "\n",
    "    for idx, current_Ixt in enumerate(Ixt):\n",
    "        current_Ity = Ity[idx]\n",
    "        ax.scatter(current_Ixt, current_Ity, label=\"layer {}\".format(idx+1))\n",
    "        ax.plot(current_Ixt, current_Ity)\n",
    "\n",
    "ax.set_xlabel(\"I(X,T)\")\n",
    "ax.set_ylabel(\"I(T,Y)\")\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from IPDL.utils import gen_log_space\n",
    "\n",
    "def show_information_plane(ip: AutoEncoderInformationPlane, reference) -> mpl.figure.Figure:\n",
    "    markers = \"o^spdP*\"\n",
    "    cmap = mpl.cm.Blues\n",
    "    # reference = MatrixBasedRenyisEntropy.entropy(ip.get_input_matrix()).cpu()\n",
    "\n",
    "    Ixt, Ity = ip.getMutualInformation(moving_average_n=2)\n",
    "\n",
    "    with plt.style.context('seaborn'):\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(16,8))\n",
    "        gs1 = fig.add_gridspec(nrows=10, ncols=2, left=0.05, right=0.84, wspace=0.05, hspace=10)\n",
    "\n",
    "        f8_ax1 = fig.add_subplot(gs1[0:9, 0])\n",
    "        f8_ax1.set_title(\"Encoder\")\n",
    "        f8_ax1.set_xlabel(\"I(X, T)\")\n",
    "        f8_ax1.set_ylabel(\"I(T, Y)\")\n",
    "        f8_ax1.set(xlim=(0, reference), ylim=(0, reference))\n",
    "        f8_ax1.plot([0, 1], [0, 1], transform=f8_ax1.transAxes, linestyle='dashed')\n",
    "\n",
    "        for idx in range((len(Ixt)//2)+1):\n",
    "            if idx == (len(Ixt)//2):\n",
    "                label = \"Bottleneck\"\n",
    "            else:\n",
    "                label = \"Encoder {}\".format(idx+1)\n",
    "            current_Ixt = np.array(Ixt[idx])\n",
    "            current_Ity = np.array(Ity[idx])\n",
    "\n",
    "            log_spaced = gen_log_space(len(current_Ixt), math.ceil(len(current_Ixt)*0.1))\n",
    "            iterations = np.arange(len(log_spaced))\n",
    "            # iterations = np.arange(len(current_Ity))\n",
    "\n",
    "            f8_ax1.scatter(current_Ixt[log_spaced], current_Ity[log_spaced], c=iterations, vmin=0, vmax=iterations.max(), label=label, marker=markers[idx], cmap=cmap, edgecolors='black')\n",
    "            f8_ax1.plot(current_Ixt[log_spaced], current_Ity[log_spaced], color=(0, 0, 0.75, 0.3))\n",
    "        f8_ax1.legend()\n",
    "\n",
    "        f8_ax2 = fig.add_subplot(gs1[0:9, 1])\n",
    "        f8_ax2.set_title(\"Decoder\")\n",
    "        f8_ax2.set_xlabel(\"I(X, T)\")\n",
    "        f8_ax2.set_ylabel(\"I(T, Y)\")\n",
    "        f8_ax2.yaxis.tick_right()\n",
    "        f8_ax2.yaxis.set_label_position(\"right\")\n",
    "        f8_ax2.set(xlim=(0, reference), ylim=(0, reference))\n",
    "        f8_ax2.plot([0, 1], [0, 1], transform=f8_ax2.transAxes, linestyle='dashed')\n",
    "\n",
    "        decode_markers = markers[:idx+1]\n",
    "        decode_markers = decode_markers[::-1]\n",
    "        for marker_idx, idx in enumerate(range((len(Ixt)//2), len(Ixt))):\n",
    "            if idx == (len(Ixt)//2):\n",
    "                label = \"Bottleneck\"\n",
    "            else:\n",
    "                label = \"Decoder {}\".format(idx+1)\n",
    "            current_Ixt = np.array(Ixt[idx])\n",
    "            current_Ity = np.array(Ity[idx])\n",
    "            log_spaced = gen_log_space(len(current_Ixt), math.ceil(len(current_Ixt)*0.1))\n",
    "            \n",
    "            marker = decode_markers[marker_idx]\n",
    "            f8_ax2.scatter(current_Ixt[log_spaced], current_Ity[log_spaced], c=iterations, vmin=0, vmax=iterations.max(), label=label, marker=marker, cmap=cmap, edgecolors='black')\n",
    "            f8_ax2.plot(current_Ixt[log_spaced], current_Ity[log_spaced], color=(0, 0, 0.75, 0.3))\n",
    "        \n",
    "        f8_ax2.legend()\n",
    "\n",
    "        f8_ax3 = fig.add_subplot(gs1[9, :])\n",
    "        f8_ax3.set_title(\"Iterations\")\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=len(current_Ixt))\n",
    "        cb1 = mpl.colorbar.ColorbarBase(f8_ax3, cmap=cmap,\n",
    "                                        norm=norm,\n",
    "                                        orientation='horizontal')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = MatrixBasedRenyisEntropy.entropy(Ax).cpu()\n",
    "fig = show_information_plane(ip, reference)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ity[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Learnable Parameters\n",
    "from torch.nn import ReLU6\n",
    "m = nn.BatchNorm1d(100)\n",
    "# Without Learnable Parameters\n",
    "m = nn.BatchNorm1d(100, affine=False)\n",
    "relu = nn.ReLU6()\n",
    "input = relu(torch.randn(20, 100))\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.nn.functional.batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.BatchNorm1d(100)\n",
    "# Without Learnable Parameters\n",
    "m = nn.BatchNorm1d(100, affine=False)\n",
    "x = torch.randn(20, 100)\n",
    "relu = nn.ReLU()\n",
    "input = relu(x)\n",
    "output = m(input)\n",
    "output_2 = torch.nn.functional.batch_norm(input, running_mean=None, running_var=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10, 100)\n",
    "norm = nn.BatchNorm1d(100)\n",
    "\n",
    "mean = a.mean(dim=0)\n",
    "std = a.std(dim=0)\n",
    "\n",
    "b = (a - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b[2, :10])\n",
    "print(norm(a)[2, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((10,100))\n",
    "b = (a - a.mean(dim=0))/torch.sqrt(a.var(dim=0, unbiased=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.mean(dim=0))\n",
    "print(b.std(dim=0))\n",
    "\n",
    "print(b[0].mean() + b[0].std())\n",
    "print(b[1].mean() + b[1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.BatchNorm1d(100, affine=False)\n",
    "c = m(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.mean(dim=0))\n",
    "print(c.std(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "j = torch.zeros(100)\n",
    "k = torch.zeros(100)\n",
    "\n",
    "d = torch.nn.functional.batch_norm(a, j, k, training=True)\n",
    "print(c[0])\n",
    "print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (a - a.mean(dim=0)) / torch.sqrt(a.var(dim=0, unbiased=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(10)\n",
    "b = a.to('cuda').clone()\n",
    "b[0] = 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc81a3ec444beb1d5a523daf231afa571e79be8a57abb6fe0028623a3d4d7136"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('DeepLearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "d715d27914ac0a4e193acd6fd2827ad289679002a3dd6b5a98aefd4aef5682a4"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
