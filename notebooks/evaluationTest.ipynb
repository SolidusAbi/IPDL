{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('DeepLearning': conda)",
   "display_name": "Python 3.7.4 64-bit ('DeepLearning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0b8b4ac5d0a82ced13579c253e3ebcf66f7b71cd01016ca2a6b9c5ad9c61843d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.split(os.getcwd())[0]\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from InformationPlane import TensorKernel, MatrixBasedRenyisEntropy, RKHSMatrixOptimizer, InformationPlane\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, ToPILImage, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorKernel:\n",
    "    '''\n",
    "        Tensor Based Radial Basis Function (RBF) Kernel\n",
    "\n",
    "        @param x\n",
    "        @param sigma\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def RBF(x: Tensor, sigma: float) -> Tensor:\n",
    "        distance = torch.cdist(x, x) \n",
    "        return torch.exp(-distance**2 / (sigma**2) )\n",
    "\n",
    "class MatrixBasedRenyisEntropy():\n",
    "    @staticmethod\n",
    "    def entropy(A: Tensor) -> float:\n",
    "        eigv = torch.symeig(A)[0].abs()\n",
    "        return -torch.sum(eigv*(torch.log2(eigv)))\n",
    "\n",
    "    @staticmethod\n",
    "    def jointEntropy(*args: Tensor) -> float:\n",
    "        for idx, val in enumerate(args):\n",
    "            if idx==0:\n",
    "                A = val.clone()\n",
    "            else:\n",
    "                A *= val\n",
    "        \n",
    "        A /= A.trace()\n",
    "        return MatrixBasedRenyisEntropy.entropy(A)\n",
    "\n",
    "    @staticmethod\n",
    "    def mutualInformation(Kx: Tensor, Ky: Tensor) -> float:\n",
    "        entropy_Ax = MatrixBasedRenyisEntropy.entropy(Kx)\n",
    "        entropy_Ay = MatrixBasedRenyisEntropy.entropy(Ky)\n",
    "        joint_entropy = MatrixBasedRenyisEntropy.jointEntropy(Kx, Ky)\n",
    "        return (entropy_Ax + entropy_Ay - joint_entropy)\n",
    "\n",
    "\n",
    "    '''\n",
    "        Generates the 'A' matrix based on RBF kernel\n",
    "\n",
    "        @return 'A' matrix\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def tensorRBFMatrix(x, sigma):\n",
    "        return TensorKernel.RBF(x, sigma) / len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RKHSMatrixOptimizer():\n",
    "    def __init__(self, beta=0.5):\n",
    "        if not(0 <= beta <= 1):\n",
    "            raise Exception('beta must be in the range [0, 1]')\n",
    "\n",
    "        self.beta = beta\n",
    "        self.sigma = None\n",
    "        self.sigma_tmp = [] #Just for saving sigma values\n",
    "\n",
    "    # Temporal, just for testing\n",
    "    def getSigmaValues(self):\n",
    "        return self.sigma_tmp\n",
    "\n",
    "    def getSigma(self):\n",
    "        return self.sigma\n",
    "\n",
    "    '''\n",
    "        @param The output of a specific layer\n",
    "        @param label_kernel_matrix\n",
    "        @param n_sigmas\n",
    "    '''\n",
    "    def step(self, layer_output: Tensor, Ky: Tensor, sigma_values: list) -> float:\n",
    "        sigma_t = self.optimize(layer_output, Ky, sigma_values)\n",
    "        self.sigma = ( (self.beta*sigma_t) + ((1-self.beta)*self.sigma) ) if not(self.sigma is None) else sigma_t\n",
    "        return self.getSigma()\n",
    "\n",
    "    '''\n",
    "        This function is used in orter to obtain the optimal kernel width for\n",
    "        an T DNN layer\n",
    "\n",
    "        @param layer_output\n",
    "        @param n_sigmas: number of possible sigma values\n",
    "\n",
    "        [Descripción del procedimiento]\n",
    "    '''\n",
    "    def optimize(self, x: Tensor, Ky: Tensor, sigma_values: list) -> float:\n",
    "        Kt = list( map(lambda sigma: TensorKernel.RBF(x, sigma).detach(), sigma_values) )\n",
    "        loss = np.array( list( map(lambda k: self.kernelAligmentLoss(k, Ky), Kt) ) )\n",
    "        self.sigma_tmp.append(sigma_values[ np.argwhere(loss == loss.max()).item(0) ])\n",
    "        return self.sigma_tmp[-1]\n",
    "\n",
    "    '''\n",
    "        Kernel Aligment Loss Function.\n",
    "\n",
    "        This function is used in order to obtain the optimal sigma parameter from\n",
    "        RBF kernel.  \n",
    "    '''\n",
    "    def kernelAligmentLoss(self, x, y):\n",
    "        return (torch.sum(x*y))/(torch.norm(x) * torch.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationPlane(torch.nn.Module):\n",
    "    '''\n",
    "        @param input_kernel: preprocessed input kernel matrix\n",
    "        @param input_kernel: preprocessed label kernel matrix\n",
    "        @param sigma_values: number of possible sigma values for optimizing process.\n",
    "        @param step: indicates the number of step for reducing the number of possible sigma values\n",
    "    '''\n",
    "    def __init__(self, mini_batch_size, beta=0.5, n_sigmas=75):\n",
    "        super(InformationPlane, self).__init__()\n",
    "\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.sigma_optimizer = RKHSMatrixOptimizer(beta)\n",
    "        self.Ixt = []\n",
    "        self.Ity = []\n",
    "\n",
    "        self.input_batch = None\n",
    "        self.label_batch = None\n",
    "        self.n_sigmas=n_sigmas\n",
    "\n",
    "    def setNumberOfSigma(self, n_sigmas):\n",
    "        self.n_sigmas = n_sigmas\n",
    "\n",
    "    '''\n",
    "        It's necessary to update the X and Y, input and label, in each iteration.\n",
    "\n",
    "        @param input: batch with the original input\n",
    "        @param label: label of the data\n",
    "    '''\n",
    "    def setInputLabel(self, inputs: Tensor, labels: Tensor):\n",
    "        self.input_batch = inputs\n",
    "        self.label_batch = labels\n",
    "\n",
    "    '''\n",
    "        @return mutual information with label {I(X,T), I(T,Y)}\n",
    "    '''\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            return x\n",
    "        \n",
    "        original_shape = x.shape\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        # Dividir en minibatchs [x]\n",
    "        # Utilizar el optimizador [x]\n",
    "        # Obtener la matrix A con el valor de sigma optimizado [x]\n",
    "        sigma_values = self.getPossibleSigmaValues(x)       \n",
    "\n",
    "        for idx in range(0, len(x), self.mini_batch_size):\n",
    "            batch = x[idx:idx+self.mini_batch_size]\n",
    "            input_batch = self.input_batch[idx:idx+self.mini_batch_size].flatten(1)\n",
    "            label_batch = self.label_batch[idx:idx+self.mini_batch_size]\n",
    "            label_kernel_matrix = TensorKernel.RBF(label_batch, 0.1)\n",
    "            \n",
    "            self.sigma_optimizer.step(batch, label_kernel_matrix, sigma_values)\n",
    "\n",
    "            A = MatrixBasedRenyisEntropy.tensorRBFMatrix(batch, self.sigma_optimizer.getSigma()).detach()\n",
    "            Ay = MatrixBasedRenyisEntropy.tensorRBFMatrix(label_batch, 0.1).detach()\n",
    "            Ax = MatrixBasedRenyisEntropy.tensorRBFMatrix(input_batch, 8).detach()\n",
    "\n",
    "            self.Ixt.append(MatrixBasedRenyisEntropy.mutualInformation(Ax, A))\n",
    "            self.Ity.append(MatrixBasedRenyisEntropy.mutualInformation(A, Ay))\n",
    "\n",
    "        x = x.reshape(original_shape)\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "        Defines an array which contains the possible sigma values in 1-D array. The number of possible\n",
    "        sigma values can be modified using the function setNumberOfSigma().\n",
    "\n",
    "        @param x: Batch tensor\n",
    "    '''\n",
    "    def getPossibleSigmaValues(self, x: Tensor) -> list:\n",
    "        distance = torch.cdist(x, x)\n",
    "        mean_distance = distance[~torch.eye(len(distance), dtype=bool)].mean().item()\n",
    "        start = (mean_distance*0.1)\n",
    "        end = (mean_distance*10)\n",
    "        return torch.arange(start, end, (end - start)/self.n_sigmas).tolist()\n",
    "\n",
    "    def moving_average(x: Tensor, n=10) -> Tensor :\n",
    "        ret = torch.cumsum(x, dtype=float)\n",
    "        ret[n:] = ret[n:] - ret[:-n]\n",
    "        return ret[n - 1:] / n\n",
    "\n",
    "    ''' \n",
    "        @return Mutual Information {I(X,T), I(T,Y)}\n",
    "    '''\n",
    "    def getMutualInformation(self):\n",
    "        return self.Ixt, self.Ity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # self.layer1_IP = InformationPlane(30, beta=0.5)\n",
    "        # self.layer2_IP = InformationPlane(30, beta=0.5)\n",
    "        # self.layer3_IP = InformationPlane(30, beta=0.5)\n",
    "        # self.layer4_IP = InformationPlane(30, beta=0.5)\n",
    "        self.layer5_IP = InformationPlane(25, beta=0.5)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(1024)\n",
    "            # self.layer1_IP,\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(1024, 20),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(20)\n",
    "            # self.layer2_IP,\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(20)\n",
    "            # self.layer3_IP,\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(20)\n",
    "            # self.layer4_IP,\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(20, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x: Tensor, labels=None) -> Tensor:\n",
    "        if not(self.training):\n",
    "            [ip.setInputLabel(x, labels) for ip in self.getInformationPlaneLayers()]\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        self.layer5_IP(self.softmax(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def getInformationPlaneLayers(self):\n",
    "        return [self.layer5_IP]\n",
    "        # return [self.layer1_IP, self.layer2_IP, self.layer3_IP, self.layer4_IP, self.layer5_IP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 14%|█▍        | 32/225 [00:00<00:05, 35.76it/s][1,    25] loss: 0.981\n",
      " 25%|██▍       | 56/225 [00:01<00:04, 36.11it/s][1,    50] loss: 0.344\n",
      " 36%|███▌      | 80/225 [00:02<00:04, 35.75it/s][1,    75] loss: 0.300\n",
      " 46%|████▌     | 104/225 [00:02<00:03, 36.36it/s][1,   100] loss: 0.241\n",
      " 59%|█████▊    | 132/225 [00:03<00:02, 37.19it/s][1,   125] loss: 0.231\n",
      " 69%|██████▉   | 156/225 [00:04<00:01, 36.61it/s][1,   150] loss: 0.198\n",
      " 80%|████████  | 180/225 [00:04<00:01, 37.39it/s][1,   175] loss: 0.206\n",
      " 91%|█████████ | 204/225 [00:05<00:00, 37.19it/s][1,   200] loss: 0.198\n",
      "100%|██████████| 225/225 [00:06<00:00, 36.52it/s][1,   225] loss: 0.171\n",
      "Finished Training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, ToPILImage, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# transformToTensor = Compose([ ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "transformToTensor = Compose([ ToTensor()])\n",
    "dataset = torchvision.datasets.MNIST(\"../datasets/MNIST/\", train=True, download=True, transform=transformToTensor)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [45000, 15000])\n",
    "dataloader = DataLoader(train_set, batch_size=200, shuffle=True, num_workers=0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.09)\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    i = 0\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = inputs.flatten(1).cuda()\n",
    "        # inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # outputs = net(inputs, one_hot(labels, num_classes=10).float())\n",
    "        outputs = net(inputs, labels.reshape((len(labels), 1)).float())\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 25 == 0:    # print every 25 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                (epoch + 1, i + 1, running_loss / 25))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "  0%|          | 0/150 [00:00&lt;?, ?it/s]\n"
    }
   ],
   "source": [
    "# from datasets.MyMNIST import MyMNIST\n",
    "\n",
    "# dataset = MyMNIST()\n",
    "# dataloader = DataLoader(dataset, batch_size=100, shuffle=False, num_workers=0)\n",
    "\n",
    "# dataset.eval()\n",
    "# net.eval()\n",
    "# with torch.no_grad():\n",
    "#     i = 0\n",
    "#     for inputs, labels in tqdm(dataloader):\n",
    "        # inputs = inputs.flatten(1).cuda()\n",
    "        # net(inputs, labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# ip = net.getInformationPlaneLayers()[0]\n",
    "# a, b = ip.getMutualInformation()\n",
    "# # print()\n",
    "# plt.plot(ip.sigma_optimizer.getSigmaValues())\n",
    "# plt.show()\n",
    "# # print(a)\n",
    "# # print(b)"
   ]
  },
  {
   "source": [
    "# Testing new Information Plane"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.load(\"tests/eval_inputs_2.pt\").flatten(1)\n",
    "labels = torch.load(\"tests/eval_labels_2.pt\")\n",
    "x_1 = net.layer1(inputs.cuda())\n",
    "x_2 = net.layer2(x_1)\n",
    "x_3 = net.layer3(x_2)\n",
    "pre_output = net.layer4(x_3)\n",
    "output = net.layer5(pre_output)\n",
    "softmax = torch.nn.Softmax()\n",
    "softmax_output = softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.1240960955619812, 0.290116548538208, 0.4561370015144348, 0.6221574544906616, 0.7881779074668884, 0.9541983604431152, 1.1202187538146973, 1.2862393856048584, 1.4522597789764404, 1.6182801723480225, 1.7843005657196045, 1.9503211975097656, 2.1163415908813477, 2.2823619842529297, 2.448382616043091, 2.614403009414673, 2.780423402786255, 2.946443796157837, 3.112464189529419, 3.27848482131958, 3.444505214691162, 3.610525608062744, 3.7765462398529053, 3.9425666332244873, 4.10858678817749, 4.274607181549072, 4.440627574920654, 4.606647968292236, 4.772668838500977, 4.938689231872559, 5.104709625244141, 5.270730018615723, 5.436750411987305, 5.602770805358887, 5.768791198730469, 5.934811592102051, 6.100831985473633, 6.266852855682373, 6.432873249053955, 6.598893642425537, 6.764914035797119, 6.930934429168701, 7.096954822540283, 7.262975215911865, 7.4289960861206055, 7.5950164794921875, 7.7610368728637695, 7.927057266235352, 8.093077659606934, 8.259098052978516, 8.425118446350098, 8.59113883972168, 8.757159233093262, 8.923179626464844, 9.089200019836426, 9.255220413208008, 9.421241760253906, 9.587262153625488, 9.75328254699707, 9.919302940368652, 10.085323333740234, 10.251343727111816, 10.417364120483398, 10.58338451385498, 10.749404907226562, 10.915425300598145, 11.081445693969727, 11.247466087341309, 11.41348648071289, 11.579506874084473, 11.745527267456055, 11.911547660827637, 12.077568054199219, 12.243589401245117, 12.4096097946167]\n"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax()\n",
    "softmax_output = softmax(output)\n",
    "n_sigmas = 75\n",
    "distance = torch.cdist(softmax_output, softmax_output)\n",
    "# distance /= distance.max()\n",
    "mean_distance = distance[~torch.eye(len(distance), dtype=bool)].mean().item()\n",
    "start = (mean_distance*0.1)\n",
    "end = (mean_distance*10)\n",
    "\n",
    "possible_sigma_values = torch.linspace(start, end, n_sigmas).tolist()\n",
    "print(possible_sigma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_loss(k_x, k_y, k_l):\n",
    "        beta = 1.0\n",
    "\n",
    "        L = torch.norm(k_l)\n",
    "        Y = torch.norm(k_y) ** beta\n",
    "        X = torch.norm(k_x) ** (1-beta)\n",
    "\n",
    "        LY = torch.trace(torch.matmul(k_l, k_y))**beta\n",
    "        LX = torch.trace(torch.matmul(k_l, k_x))**(1-beta)\n",
    "\n",
    "        return 2*torch.log2((LY*LX)/(L*Y*X))\n",
    "\n",
    "def kernelAligmentLoss(x, y) -> float:\n",
    "        return (torch.sum(x*y)/(torch.norm(x) * torch.norm(y))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mutual Information Kt_1\ntensor(5.4736, device='cuda:0', grad_fn=<SubBackward0>)\ntensor(3.2971, device='cuda:0', grad_fn=<SubBackward0>)\nMutual Information Kt_2\ntensor(5.3874, device='cuda:0', grad_fn=<SubBackward0>)\ntensor(3.2959, device='cuda:0', grad_fn=<SubBackward0>)\nMutual Information Kt\ntensor(3.4205, device='cuda:0', grad_fn=<SubBackward0>)\ntensor(3.2223, device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Kx = TensorKernel.RBF(inputs[0:], 8) / 100\n",
    "Ky = TensorKernel.RBF(labels[0:], 0.1) / 100\n",
    "Kt_1 = TensorKernel.RBF(x_1[0:], 15) / 100\n",
    "Kt_2 = TensorKernel.RBF(x_2[0:], 1.5) / 100\n",
    "Kt = TensorKernel.RBF(softmax_output[0:], 0.1) / 100\n",
    "\n",
    "entropy_kx = MatrixBasedRenyisEntropy.entropy(Kx.cuda())\n",
    "entropy_ky = MatrixBasedRenyisEntropy.entropy(Ky.cuda())\n",
    "entropy_kt = MatrixBasedRenyisEntropy.entropy(Kt.cuda())\n",
    "entropy_kt_1 = MatrixBasedRenyisEntropy.entropy(Kt_1.cuda())\n",
    "entropy_kt_2 = MatrixBasedRenyisEntropy.entropy(Kt_2.cuda())\n",
    "joint_entropy_kxkt = MatrixBasedRenyisEntropy.jointEntropy(Kx.cuda(), Kt.cuda())\n",
    "joint_entropy_kxkt_1 = MatrixBasedRenyisEntropy.jointEntropy(Kx.cuda(), Kt_1.cuda())\n",
    "joint_entropy_kxkt_2 = MatrixBasedRenyisEntropy.jointEntropy(Kx.cuda(), Kt_2.cuda())\n",
    "joint_entropy_ktky = MatrixBasedRenyisEntropy.jointEntropy(Kt.cuda(), Ky.cuda())\n",
    "joint_entropy_kt_1ky = MatrixBasedRenyisEntropy.jointEntropy(Kt_1.cuda(), Ky.cuda())\n",
    "joint_entropy_kt_2ky = MatrixBasedRenyisEntropy.jointEntropy(Kt_2.cuda(), Ky.cuda())\n",
    "\n",
    "# print(entropy_kx)\n",
    "# print(entropy_ky)\n",
    "# print(entropy_kt)\n",
    "# print(joint_entropy_kxkt)\n",
    "# print(joint_entropy_ktky)\n",
    "\n",
    "print(\"Mutual Information Kt_1\")\n",
    "print(entropy_kx + entropy_kt_1 - joint_entropy_kxkt_1)\n",
    "print(entropy_kt_1 + entropy_ky - joint_entropy_kt_1ky)\n",
    "\n",
    "print(\"Mutual Information Kt_2\")\n",
    "print(entropy_kx + entropy_kt_2 - joint_entropy_kxkt_2)\n",
    "print(entropy_kt_2 + entropy_ky - joint_entropy_kt_2ky)\n",
    "\n",
    "print(\"Mutual Information Kt\")\n",
    "print(entropy_kx + entropy_kt - joint_entropy_kxkt)\n",
    "print(entropy_kt + entropy_ky - joint_entropy_ktky)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A\nB\nC\nD\nMutual Information Kt_1\ntensor(4.3471, device='cuda:0', grad_fn=<SubBackward0>)\ntensor(3.1829, device='cuda:0', grad_fn=<SubBackward0>)\nMutual Information Kt\ntensor(3.0835, device='cuda:0', grad_fn=<SubBackward0>)\ntensor(3.0981, device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "length = 30\n",
    "Kx = TensorKernel.RBF(inputs[0:length], 8) / length\n",
    "Ky = TensorKernel.RBF(labels[0:length], 0.1) / length\n",
    "Kt_1 = TensorKernel.RBF(x_1[0:length], 15.8) / length\n",
    "Kt = TensorKernel.RBF(softmax_output[0:length], 0.6) / length\n",
    "\n",
    "print(\"A\")\n",
    "entropy_kx = MatrixBasedRenyisEntropy.entropy(Kx.cuda())\n",
    "print(\"B\")\n",
    "entropy_ky = MatrixBasedRenyisEntropy.entropy(Ky.cuda())\n",
    "print(\"C\")\n",
    "entropy_kt = MatrixBasedRenyisEntropy.entropy(Kt.cuda())\n",
    "print(\"D\")\n",
    "entropy_kt_1 = MatrixBasedRenyisEntropy.entropy(Kt_1.cuda())\n",
    "joint_entropy_kxkt = MatrixBasedRenyisEntropy.jointEntropy(Kx.cuda(), Kt.cuda())\n",
    "joint_entropy_kxkt_1 = MatrixBasedRenyisEntropy.jointEntropy(Kx.cuda(), Kt_1.cuda())\n",
    "joint_entropy_ktky = MatrixBasedRenyisEntropy.jointEntropy(Kt.cuda(), Ky.cuda())\n",
    "joint_entropy_kt_1ky = MatrixBasedRenyisEntropy.jointEntropy(Kt_1.cuda(), Ky.cuda())\n",
    "\n",
    "# print(entropy_kx)\n",
    "# print(entropy_ky)\n",
    "# print(entropy_kt)\n",
    "# print(joint_entropy_kxkt)\n",
    "# print(joint_entropy_ktky)\n",
    "\n",
    "print(\"Mutual Information Kt_1\")\n",
    "print(entropy_kx + entropy_kt_1 - joint_entropy_kxkt_1)\n",
    "print(entropy_kt_1 + entropy_ky - joint_entropy_kt_1ky)\n",
    "\n",
    "print(\"Mutual Information Kt\")\n",
    "print(entropy_kx + entropy_kt - joint_entropy_kxkt)\n",
    "print(entropy_kt + entropy_ky - joint_entropy_ktky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1240960955619812\ntorch.Size([30, 30])\ntensor(30)\ntorch.Size([30, 30])\ntensor(30)\ntorch.Size([30, 30])\ntensor(28, device='cuda:0')\ntensor(3.1974, device='cuda:0')\ntensor(3.1661, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax()\n",
    "softmax_output = softmax(output)\n",
    "with torch.no_grad():\n",
    "    Ky = TensorKernel.RBF(labels[0:30], 0.1).cuda()\n",
    "    Kx = TensorKernel.RBF(inputs[0:30], 8).cuda()\n",
    "    Kts = list( map(lambda sigma: TensorKernel.RBF(softmax_output[0:30], sigma).cuda(), possible_sigma_values) )\n",
    "    loss = np.array( list( map(lambda k: kernelAligmentLoss(k, Ky), Kts) ) )\n",
    "    # loss = np.array( list( map(lambda k: kernel_loss(Kx, Ky, k), Kts) ) )\n",
    "    best_sigma = possible_sigma_values[ (np.argwhere(loss == loss.max())).item() ]\n",
    "    # print(loss)\n",
    "    # print(possible_sigma_values)\n",
    "    print(best_sigma)\n",
    "    # self.sigma_tmp.append(sigma_values[ np.argwhere(loss == loss.max()).item(0) ])\n",
    "\n",
    "    # Ay = MatrixBasedRenyisEntropy.tensorRBFMatrix(labels[0:30], 0.1)\n",
    "    # Ax = MatrixBasedRenyisEntropy.tensorRBFMatrix(inputs[0:30], 8)\n",
    "    # A = MatrixBasedRenyisEntropy.tensorRBFMatrix(softmax_output[0:30], best_sigma)\n",
    "    \n",
    "    Ay = TensorKernel.RBF(labels[0:30], 0.1) / 30\n",
    "    Ax = TensorKernel.RBF(inputs[0:30], 8) / 30\n",
    "    A = TensorKernel.RBF(softmax_output[0:30], best_sigma) / 30\n",
    "\n",
    "    print(Ay.shape)\n",
    "    print(torch.matrix_rank(Ay))\n",
    "    print(Ax.shape)\n",
    "    print(torch.matrix_rank(Ax))\n",
    "    print(A.shape)\n",
    "    print(torch.matrix_rank(A))\n",
    "\n",
    "    print(MatrixBasedRenyisEntropy.mutualInformation(Ax.cuda(), A.cuda()))\n",
    "    print(MatrixBasedRenyisEntropy.mutualInformation(A.cuda(), Ay.cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_mat(x, y=None):\n",
    "    try:\n",
    "        x = torch.from_numpy(x)\n",
    "    except TypeError:\n",
    "        x = x\n",
    "\n",
    "    dist = torch.norm(x[:, None] - x, dim=2, p=2)\n",
    "    return dist / dist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance_1 = dist_mat(pre_output)\n",
    "distance_2 = torch.cdist(pre_output, pre_output)\n",
    "print(torch.allclose(distance_1, distance_2/distance_2.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distance = torch.cdist(pre_output, pre_output, compute_mode='use_mm_for_euclid_dist') \n",
    "distance = distance / distance.max()\n",
    "print(distance[:2])\n",
    "print(distance_1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = dist_mat(softmax_output)\n",
    "print(k.shape)\n",
    "print(torch.sort(k)[0][:, :50].shape)\n",
    "sigma = torch.sort(k)[0][:, :50].mean()\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "Tensor([10]).cuda().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}