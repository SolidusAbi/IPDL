{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.split(os.getcwd())[0]\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from IPDL import TensorKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatrixEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixEstimator(nn.Module):\n",
    "    def __init__(self, sigma = 0.1):\n",
    "        super(MatrixEstimator, self).__init__()\n",
    "        \n",
    "        self.sigma = nn.Parameter(torch.tensor(sigma), requires_grad=False)\n",
    "        self.x = torch.rand((10, 1))\n",
    "\n",
    "    def set_sigma(self, sigma: float) -> None:\n",
    "        self.sigma.data = torch.tensor(sigma)\n",
    "\n",
    "    def get_sigma(self) -> float:\n",
    "        return self.sigma.data.item()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if not self.training:\n",
    "            self.x = x.detach().cpu() # To CPU in order to save memory on GPU\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_matrix(self, activation= None) -> Tensor:\n",
    "        '''\n",
    "            Return matrix A\n",
    "\n",
    "            @param activation: you can chose any nn.Module if is necesary to apply an activation. \n",
    "                It is usefull if you need include an activation at the end of your model to compute \n",
    "                the Information Plane.\n",
    "        '''\n",
    "        device = self.sigma.device # To the device where parameters are located\n",
    "        n = self.x.size(0)\n",
    "        \n",
    "        if not(activation is None):\n",
    "            return (TensorKernel.RBF(activation(self.x).flatten(1).to(device), self.sigma) / n)\n",
    "        else:\n",
    "            return (TensorKernel.RBF(self.x.flatten(1).to(device), self.sigma) / n)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"MatrixEstimator(sigma={:.2f})\".format(self.sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixEstimatorTest():\n",
    "    model = nn.Sequential(\n",
    "        nn.Identity(),\n",
    "        MatrixEstimator(8),\n",
    "        nn.Identity(),\n",
    "        MatrixEstimator(0.5),\n",
    "        nn.Identity(),\n",
    "        MatrixEstimator(0.100001)\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.rand(10, 10)\n",
    "    y = model(x)\n",
    "\n",
    "    Ax_0 = TensorKernel.RBF(x, sigma=8) / x.size(0)\n",
    "    Ax_1 = TensorKernel.RBF(x, sigma=0.5) / x.size(0)\n",
    "    Ax_2 = TensorKernel.RBF(x, sigma=0.100001) / x.size(0)\n",
    "\n",
    "    case_0 = all((model[1].get_matrix() == Ax_0).flatten())\n",
    "    print(\"Test 0: {}\".format(case_0))\n",
    "\n",
    "    case_1 = all((model[3].get_matrix() == Ax_1).flatten())\n",
    "    print(\"Test 1: {}\".format(case_1))\n",
    "\n",
    "    case_2 = all((model[5].get_matrix() == Ax_2).flatten())\n",
    "    print(\"Test 2: {}\".format(case_2))\n",
    "    \n",
    "\n",
    "MatrixEstimatorTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClassificationInformationPlane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPDL import MatrixBasedRenyisEntropy as renyis\n",
    "\n",
    "class InformationPlane():\n",
    "    def __init__(self):\n",
    "        self.Ixt = []\n",
    "        self.Ity = []\n",
    "\n",
    "    def getMutualInformation(self, moving_average_n = 0):\n",
    "        if moving_average_n == 0:\n",
    "            return self.Ixt, self.Ity\n",
    "        else:\n",
    "            filter_Ixt = list(map(lambda Ixt: mva(Ixt, moving_average_n), self.Ixt))\n",
    "            filter_Ity = list(map(lambda Ity: mva(Ity, moving_average_n), self.Ity))\n",
    "            return filter_Ixt, filter_Ity\n",
    "        \n",
    "    def computeMutualInformation(self, Ax: Tensor, Ay: Tensor):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "\n",
    "\n",
    "class ClassificationInformationPlane(InformationPlane):\n",
    "    '''\n",
    "        Pass a list of tensor which contents the matrices in order to calculate the\n",
    "        MutualInformation\n",
    "\n",
    "        IP implementaiton that works for classification problems.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: nn.Module, use_softmax=True):\n",
    "        '''\n",
    "            @param model: model where \n",
    "            @param softmax: include a softmax layer at the end of the model. It is usefull \n",
    "                if your model does not contain this layer. \n",
    "        '''\n",
    "        super(ClassificationInformationPlane, self).__init__()\n",
    "        self.matrices_per_layers = []\n",
    "        self.use_softmax = use_softmax\n",
    "      \n",
    "        # First element corresponds to input A matrix and last element\n",
    "        # is the output A matrix\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, (MatrixEstimator)):\n",
    "                self.matrices_per_layers.append(module)\n",
    "\n",
    "        # self.Ixt = []\n",
    "        # self.Ity = []\n",
    "        for i in range(len(self.matrices_per_layers)):\n",
    "            self.Ixt.append([])\n",
    "            self.Ity.append([])\n",
    "    \n",
    "    def computeMutualInformation(self, Ax: Tensor, Ay: Tensor):\n",
    "        for idx, matrix_estimator in enumerate(self.matrices_per_layers):\n",
    "            activation = None \n",
    "            if self.use_softmax and idx == len(self.matrices_per_layers)-1 :\n",
    "                activation = nn.Softmax(dim=1) \n",
    "            \n",
    "            # if isinstance(activation, nn.Softmax):\n",
    "            #     print(\"Softmax in layer {}\".format(idx))\n",
    "\n",
    "            Ixt = renyis.mutualInformation(Ax, matrix_estimator.get_matrix(activation)).cpu()\n",
    "            Ity = renyis.mutualInformation(matrix_estimator.get_matrix(activation), Ay).cpu()\n",
    "\n",
    "            self.Ixt[idx].append(Ixt)\n",
    "            self.Ity[idx].append(Ity)\n",
    "\n",
    "    # def getMutualInformation(self, moving_average_n = 0):\n",
    "    #     if moving_average_n == 0:\n",
    "    #         return self.Ixt, self.Ity\n",
    "    #     else:\n",
    "    #         filter_Ixt = list(map(lambda Ixt: mva(Ixt, moving_average_n), self.Ixt))\n",
    "    #         filter_Ity = list(map(lambda Ity: mva(Ity, moving_average_n), self.Ity))\n",
    "    #         return filter_Ixt, filter_Ity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatrixOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixOptimizer():\n",
    "    def __init__(self, model: nn.Module, beta=0.5, n_sigmas=300):\n",
    "        if not(0 <= beta <= 1):\n",
    "            raise Exception('beta must be in the range [0, 1]')\n",
    "\n",
    "        self.matrix_estimators = []\n",
    "        # First element corresponds to input A matrix and last element\n",
    "        # is the output A matrix\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, (MatrixEstimator)):\n",
    "                self.matrix_estimators.append(module)\n",
    "\n",
    "        self.beta = beta\n",
    "        self.n_sigmas = n_sigmas\n",
    "\n",
    "        #just for debugging\n",
    "        self.sigma_prev = [-1] * len(self.matrix_estimators)\n",
    "        for idx, matrix_estimator in enumerate(self.matrix_estimators):\n",
    "            self.sigma_prev[idx] = [matrix_estimator.get_sigma()]\n",
    "\n",
    "    def step(self, Ky: Tensor) -> None:\n",
    "        self.optimize(Ky)\n",
    "    \n",
    "    def optimize(self, Ky: Tensor) -> None:\n",
    "        '''\n",
    "            This function is used in orter to obtain the optimal kernel width for\n",
    "            an T DNN layer\n",
    "\n",
    "            @param layer_output\n",
    "            @param n_sigmas: number of possible sigma values\n",
    "\n",
    "            [DescripciÃ³n del procedimiento]\n",
    "        '''\n",
    "        # Ky = Ky.to(self.device)\n",
    "        device = Ky.device\n",
    "        for idx, matrix_estimator in enumerate(self.matrix_estimators):\n",
    "            activation = nn.Softmax(dim=1) if idx == len(self.matrix_estimators)-1 else nn.Identity()\n",
    "\n",
    "            x = activation(matrix_estimator.x).to(device)\n",
    "            sigma_values = self.getPossibleSigmaValues(x, self.n_sigmas)\n",
    "\n",
    "            Kt = list( map(lambda sigma: TensorKernel.RBF(x, sigma), sigma_values) )    \n",
    "            \n",
    "            loss = np.array( list( map(lambda k: self.kernelAligmentLoss(k, Ky), Kt) ) )\n",
    "            best_sigma = sigma_values[ np.argwhere(loss == loss.max()).item(0) ]\n",
    "            \n",
    "            best_sigma = ( (self.beta*best_sigma) + ((1-self.beta)*matrix_estimator.get_sigma()) )\n",
    "            # if self.sigma_prev[idx]:\n",
    "            # best_sigma = ( (self.beta*best_sigma) + ((1-self.beta)*self.sigma_prev[idx][-1]) ) \n",
    "            \n",
    "            #Just for debugging\n",
    "            self.sigma_prev[idx].append(best_sigma)\n",
    "\n",
    "            matrix_estimator.set_sigma(best_sigma)\n",
    "\n",
    "        # Ky = Ky.cpu()\n",
    "\n",
    "    \n",
    "    def kernelAligmentLoss(self, Kx: Tensor, Ky: Tensor) -> float:\n",
    "        '''\n",
    "            Kernel Aligment Loss Function.\n",
    "\n",
    "            This function is used in order to obtain the optimal sigma parameter from\n",
    "            RBF kernel.  \n",
    "        '''\n",
    "        return (torch.sum(Kx*Ky)/(torch.norm(Kx) * torch.norm(Ky))).item()\n",
    "\n",
    "    \n",
    "    def getPossibleSigmaValues(self, x: Tensor, n=100) -> Tensor:\n",
    "        '''\n",
    "            Obtener los sigma values para el optimizador\n",
    "        '''\n",
    "        distance = torch.cdist(x,x)\n",
    "        distance = distance[torch.triu(torch.ones(distance.shape, dtype=torch.bool), diagonal=1)] \n",
    "        return torch.linspace(0.1, 10*distance.mean(), n).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def MatrixOptimizerTest():\n",
    "    model = nn.Sequential(\n",
    "        nn.Identity(),\n",
    "        MatrixEstimator(8),\n",
    "        nn.Identity(),\n",
    "        MatrixEstimator(0.1),\n",
    "        nn.Identity(),\n",
    "        MatrixEstimator(0.1)\n",
    "    )\n",
    "    print(model)\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.rand(10, 10)\n",
    "    \n",
    "    mask = np.random.randint(0,2,size=x[:,0].shape).astype(np.bool) \n",
    "    y = torch.zeros(10, 2)\n",
    "    y[mask, 0] = 1\n",
    "    y[np.invert(mask), 1] = 1\n",
    "\n",
    "    matrix_optimizer = MatrixOptimizer(model)\n",
    "    \n",
    "\n",
    "    output = model(x)\n",
    "    \n",
    "    Ky = TensorKernel.RBF(y, sigma=0.1)\n",
    "    matrix_optimizer.optimize(Ky)\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "\n",
    "MatrixOptimizerTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(1024, 20),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(20),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(20, 20),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(20),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(20, 20),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.BatchNorm1d(20),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(20, 10),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            MatrixEstimator(0.1),\n",
    "        )\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weight_init(m)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def weight_init(self, module):\n",
    "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(module.weight.data, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.nn.functional import one_hot\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP().to(device)\n",
    "matrix_optimizer = MatrixOptimizer(net, beta=0.1, n_sigmas=150)\n",
    "ip = ClassificationInformationPlane(net, use_softmax=True)\n",
    "\n",
    "transformToTensor = Compose([ ToTensor() ])\n",
    "dataset = torchvision.datasets.MNIST(\"../datasets/MNIST/\", train=True, download=True, transform=transformToTensor)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [59850 , 150])\n",
    "dataloader = DataLoader(train_set, batch_size=120, shuffle=True, num_workers=0)\n",
    "dataloader_2 = DataLoader(val_set, batch_size=150, shuffle=False, num_workers=0)\n",
    "\n",
    "val_inputs, val_labels = next(iter(dataloader_2))\n",
    "val_inputs = val_inputs.flatten(1).to(device)\n",
    "val_labels = one_hot(val_labels, num_classes=10).float().to(device)\n",
    "\n",
    "Kx = TensorKernel.RBF(val_inputs, sigma=8).to(device)\n",
    "Ky = TensorKernel.RBF(val_labels, sigma=0.1).to(device)\n",
    "Ax = Kx/val_inputs.size(0)\n",
    "Ay = Ky/val_labels.size(0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.009, momentum=0.9)\n",
    "\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        net(val_inputs)\n",
    "        \n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        inputs = inputs.flatten(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        net.train()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 50 == 0:\n",
    "            loss_record.append(running_loss / 50)\n",
    "            running_loss = 0.0\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            net(val_inputs)\n",
    "            matrix_optimizer.step(Ky)\n",
    "            ip.computeMutualInformation(Ax.cpu(), Ay.cpu())\n",
    "\n",
    "        if i > 500:\n",
    "            for ip in net.getInformationPlaneLayers():\n",
    "                ip.setNumberOfSigma(100)\n",
    "\n",
    "        i += 1\n",
    " \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import moving_average, showMutualInformation\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ixts, Itys = ip.getMutualInformation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=['Reds', 'Blues', 'binary', 'Oranges', 'Greens']\n",
    "with plt.style.context('seaborn'):\n",
    "    fig = plt.figure(constrained_layout=False)\n",
    "    gs1 = fig.add_gridspec(nrows=1, ncols=1, left=0.05, right=0.84, wspace=0.05)\n",
    "    gs2 = fig.add_gridspec(nrows=1, ncols=len(Ixts), left=0.85, right=0.95, wspace=0)\n",
    "    f8_ax1 = fig.add_subplot(gs1[:, :])\n",
    "    f8_ax1.set_xlabel(\"I(X, T)\")\n",
    "    f8_ax1.set_ylabel(\"I(T, Y)\")\n",
    "\n",
    "    for idx, Ixt in enumerate(Ixts):\n",
    "        Ity = Itys[idx]\n",
    "        cmap = plt.cm.get_cmap(colors[idx])\n",
    "        # Ixt, Ity = ip.getMutualInformation()\n",
    "        Ixt = moving_average(Ixt, n=10)\n",
    "        Ity = moving_average(Ity, n=10)\n",
    "        iterations = np.arange(len(Ixt))\n",
    "        color = np.array([cmap(iterations[-1])])\n",
    "        sc = f8_ax1.scatter(Ixt, Ity, c=iterations, vmin=0, vmax=iterations.max(), cmap=cmap, edgecolor=color)\n",
    "        f8_ax1.scatter([], [], c=color, label=\"Layer {}\".format(idx))\n",
    "\n",
    "        f8_ax2 = fig.add_subplot(gs2[0, idx])\n",
    "        cb = fig.colorbar(sc, cax=f8_ax2, pad=0)\n",
    "        cb.set_ticks([])\n",
    "\n",
    "    f8_ax1.legend()\n",
    "    cb.set_ticks([0, iterations.max()])\n",
    "    cb.set_label(\"Iterations\", labelpad=-18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "sigma_per_layers = matrix_optimizer.sigma_prev\n",
    "\n",
    "for idx, sigmas in enumerate(sigma_per_layers):\n",
    "    plt.plot(sigmas, label=\"Layer {}\".format(idx))\n",
    "\n",
    "plt.title(\"Sigma values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ky.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('DeepLearning': conda)",
   "name": "python385jvsc74a57bd00b8b4ac5d0a82ced13579c253e3ebcf66f7b71cd01016ca2a6b9c5ad9c61843d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
