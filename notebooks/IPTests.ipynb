{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.path.split(os.getcwd())[0]\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, ToPILImage, Normalize\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import random\n",
    "\n",
    "\n",
    "from InformationPlane import TensorKernel, MatrixBasedRenyisEntropy, RKHSMatrixOptimizer, InformationPlane\n",
    "from models.Paper import CNN\n",
    "from datasets.MyMNIST import MyMNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformToTensor = Compose([ ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "# # transformToTensor = Compose([ ToTensor()])\n",
    "\n",
    "transformToImage = Compose([  Normalize((-0.1307/0.3081,), (1/0.3081,)), ToPILImage()])\n",
    "# # transformToImage = Compose([ ToPILImage()])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\"../datasets/MNIST/\", train=True, download=True, transform=transformToTensor)\n",
    "\n",
    "dataset2 = MyMNIST()\n",
    "dataset2.eval()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=100, shuffle=False, num_workers=0)\n",
    "dataloader2 = DataLoader(dataset2, batch_size=100, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for input, label in dataloader:\n",
    "    plt.imshow(transformToImage(input[0]), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information plane test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RKHSMatrixOptimizer():\n",
    "    def __init__(self, beta=0.5):\n",
    "        if not(0 <= beta <= 1):\n",
    "            raise Exception('beta must be in the range [0, 1]')\n",
    "\n",
    "        self.beta = beta\n",
    "        self.sigma = None\n",
    "        self.sigma_tmp = [] #Just for saving sigma values\n",
    "\n",
    "    # Temporal, just for testing\n",
    "    def getSigmaValues(self):\n",
    "        return self.sigma_tmp\n",
    "\n",
    "    def getSigma(self):\n",
    "        return self.sigma\n",
    "\n",
    "    '''\n",
    "        @param The output of a specific layer\n",
    "        @param label_kernel_matrix\n",
    "        @param n_sigmas\n",
    "    '''\n",
    "    def step(self, layer_output, Ky, n_sigmas=75):\n",
    "        sigma_t = self.optimize(layer_output, Ky, n_sigmas)\n",
    "        self.sigma = ( (self.beta*sigma_t) + ((1-self.beta)*self.sigma) ) if not(self.sigma is None) else sigma_t\n",
    "        return self.getSigma()\n",
    "\n",
    "    '''\n",
    "        This function is used in orter to obtain the optimal kernel width for\n",
    "        an T DNN layer\n",
    "\n",
    "        @param layer_output\n",
    "        @param n_sigmas: number of possible sigma values\n",
    "\n",
    "        [DescripciÃ³n del procedimiento]\n",
    "    '''\n",
    "    def optimize(self, layer_output, Ky, n_sigmas):\n",
    "        distance = torch.cdist(layer_output, layer_output)\n",
    "        mean_distance = distance[distance != 0].mean().detach().cpu()\n",
    "        sigma_values = np.arange(mean_distance*0.1, mean_distance*10, (mean_distance*10 - mean_distance*0.1)/n_sigmas)\n",
    "\n",
    "        Kt = list( map(lambda sigma: TensorKernel.RBF(layer_output, sigma).detach(), sigma_values.tolist()) )\n",
    "        loss = np.array( list( map(lambda k: self.kernelAligmentLoss(k, Ky), Kt) ) )\n",
    "        self.sigma_tmp.append(sigma_values[ np.argwhere(loss == loss.max()).item(0) ])\n",
    "        return sigma_values[ np.argwhere(loss == loss.max()).item(0) ]\n",
    "\n",
    "    '''\n",
    "        Kernel Aligment Loss Function.\n",
    "\n",
    "        This function is used in order to obtain the optimal sigma parameter from\n",
    "        RBF kernel.  \n",
    "    '''\n",
    "    def kernelAligmentLoss(self, x, y):\n",
    "        return (torch.sum(x*y))/(torch.norm(x) * torch.norm(y))\n",
    "\n",
    "\n",
    "'''\n",
    "    Information Plane is just generated using evaluation process.\n",
    "'''\n",
    "class InformationPlane(torch.nn.Module):\n",
    "    '''\n",
    "        @param input_kernel: preprocessed input kernel matrix\n",
    "        @param input_kernel: preprocessed label kernel matrix\n",
    "        @param sigma_values: number of possible sigma values for optimizing process.\n",
    "        @param step: indicates the number of step for reducing the number of possible sigma values\n",
    "    '''\n",
    "    def __init__(self, beta=0.5):\n",
    "        super(InformationPlane, self).__init__()\n",
    "\n",
    "        self.sigma_optimizer = RKHSMatrixOptimizer(beta)\n",
    "        self.Ixt = []\n",
    "        self.Ity = []\n",
    "\n",
    "        self.input_batch = None\n",
    "        self.label_batch = None\n",
    "\n",
    "    '''\n",
    "        It's necessary to update the X and Y, input and label, in each iteration.\n",
    "\n",
    "        @param input: batch with the original input\n",
    "        @param label: label of the data\n",
    "    '''\n",
    "    def setInputLabel(self, input, label):\n",
    "        self.input_batch = input\n",
    "        self.label_batch = label\n",
    "\n",
    "    '''\n",
    "        @param beta regularizer term to stabilize the optimal sigma value across the previous iteration\n",
    "        \n",
    "        @return mutual information with label {I(X,T), I(T,Y)}\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return x\n",
    "\n",
    "        original_shape = x.shape\n",
    "        x = x.flatten(1)\n",
    "        label_kernel_matrix = TensorKernel.RBF(self.label_batch, 0.1)\n",
    "        self.sigma_optimizer.step(x, label_kernel_matrix, 75)\n",
    "\n",
    "        A = MatrixBasedRenyisEntropy.tensorRBFMatrix(x, self.sigma_optimizer.getSigma()).detach()\n",
    "        Ay = MatrixBasedRenyisEntropy.tensorRBFMatrix(self.label_batch, 0.1).detach()\n",
    "        Ax = MatrixBasedRenyisEntropy.tensorRBFMatrix(self.input_batch.flatten(1), 8).detach()\n",
    "\n",
    "        self.Ixt.append(MatrixBasedRenyisEntropy.mutualInformation(Ax, A)) \n",
    "        self.Ity.append(MatrixBasedRenyisEntropy.mutualInformation(A, Ay))\n",
    "\n",
    "        x = x.reshape(original_shape)\n",
    "        return x\n",
    "    \n",
    "    # def forward(self, x, input, label):\n",
    "    #     original_shape = x.shape\n",
    "    #     x = x.flatten(1)\n",
    "        \n",
    "    #     label_kernel_matrix = TensorKernel.RBF(label, 0.1)\n",
    "    #     self.sigma_optimizer.step(x, label_kernel_matrix, 75)\n",
    "\n",
    "    #     A = MatrixBasedRenyisEntropy.tensorRBFMatrix(x, self.sigma_optimizer.getSigma()).detach()\n",
    "    #     Ay = MatrixBasedRenyisEntropy.tensorRBFMatrix(label, 0.1).detach()\n",
    "    #     Ax = MatrixBasedRenyisEntropy.tensorRBFMatrix(input.flatten(1), 8).detach()\n",
    "\n",
    "    #     self.Ixt.append(MatrixBasedRenyisEntropy.mutualInformation(Ax, A)) \n",
    "    #     self.Ity.append(MatrixBasedRenyisEntropy.mutualInformation(A, Ay))\n",
    "\n",
    "    #     x = x.reshape(original_shape)\n",
    "    #     return x\n",
    "\n",
    "    ''' \n",
    "        @return Mutual Information {I(X,T), I(T,Y)}\n",
    "    '''\n",
    "    def getMutualInformation(self):\n",
    "        return self.Ixt, self.Ity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.layer1_IP = InformationPlane(beta=0.5)\n",
    "        self.layer2_IP = InformationPlane(beta=0.5)\n",
    "        self.layer3_IP = InformationPlane(beta=0.5)\n",
    "        self.layer4_IP = InformationPlane(beta=0.5)\n",
    "        self.fc_IP = InformationPlane(beta=0.5)\n",
    "\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, 3, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self.layer1_IP,     \n",
    "            nn.BatchNorm2d(4)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(4, 8, 3, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self.layer2_IP,\n",
    "            nn.BatchNorm2d(8),      \n",
    "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, 3, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self.layer3_IP,\n",
    "            nn.BatchNorm2d(16),         \n",
    "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(400, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self.layer4_IP,\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        [ip.setInputLabel(x, label) for ip in self.getInformationPlaneLayers()]\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        self.fc_IP(self.softmax(x))\n",
    "        return x\n",
    "\n",
    "    def getInformationPlaneLayers(self):\n",
    "        return [self.layer1_IP, self.layer2_IP, self.layer3_IP, self.layer4_IP, self.fc_IP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer1_IP = InformationPlane(beta=0.5)\n",
    "        self.layer2_IP = InformationPlane(beta=0.5)\n",
    "        self.layer3_IP = InformationPlane(beta=0.5)\n",
    "        self.layer4_IP = InformationPlane(beta=0.5)\n",
    "        self.layer5_IP = InformationPlane(beta=0.5)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # self.layer1_IP,\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 20),\n",
    "            nn.ReLU(inplace=True)\n",
    "            # self.layer2_IP,\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # self.layer3_IP,\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # self.layer4_IP,\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.Linear(20, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        # [ip.setInputLabel(x, label) for ip in self.getInformationPlaneLayers()]\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        # self.layer5_IP(self.softmax(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_mi(self, x, label):\n",
    "        [ip.setInputLabel(x, label) for ip in self.getInformationPlaneLayers()]\n",
    "        x = self.layer1(x)\n",
    "        self.layer1_IP(x)\n",
    "        x = self.layer2(x)\n",
    "        self.layer2_IP(x)\n",
    "        x = self.layer3(x)\n",
    "        self.layer3_IP(x)\n",
    "        x = self.layer4(x)\n",
    "        self.layer4_IP(x)\n",
    "        x = self.layer5(x)\n",
    "        self.layer5_IP(self.softmax(x))\n",
    "\n",
    "    def getInformationPlaneLayers(self):\n",
    "        return [self.layer1_IP, self.layer2_IP, self.layer3_IP, self.layer4_IP, self.layer5_IP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = CNN().cuda()\n",
    "net = MLP().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.009, momentum=0.9)\n",
    "\n",
    "def train():\n",
    "    for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        net.train()\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs = inputs.flatten(1).cuda()\n",
    "            # inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            # outputs = net(inputs, one_hot(labels, num_classes=10).float())\n",
    "            outputs = net(inputs, labels.reshape((len(labels), 1)).float())\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # for ip in net.getInformationPlaneLayers():\n",
    "            #     Ixt, Ity = ip.getMutualInformation()\n",
    "            #     if math.isnan(Ixt[-1]) or math.isnan(Ity[-1]):\n",
    "            #         print(\"IP: Error!\")\n",
    "            #         return inputs, labels\n",
    "\n",
    "            # print statistics\n",
    "            # running_loss += loss.item()\n",
    "            # if (i+1) % 25 == 0:    # print every 25 mini-batches\n",
    "            #     print('[%d, %5d] loss: %.3f' %\n",
    "            #         (epoch + 1, i + 1, running_loss / 25))\n",
    "            #     running_loss = 0.0\n",
    "        \n",
    "        # net.eval()\n",
    "        # for inputs, labels in dataloader2:\n",
    "        #     inputs = inputs.flatten(1).cuda()\n",
    "        #     net.compute_mi(inputs, labels)\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# inputs, labels = train()\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ips = net.getInformationPlaneLayers()[0]\n",
    "ips.eval()\n",
    "print(ips.training)\n",
    "ips.train()\n",
    "print(ips.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mpl.style.use('seaborn')\n",
    "plt.scatter(moving_average(net.layer1_IP.getMutualInformation()[0]), moving_average(net.layer1_IP.getMutualInformation()[1]), label=\"Layer 1\")\n",
    "plt.scatter(moving_average(net.layer2_IP.getMutualInformation()[0]), moving_average(net.layer2_IP.getMutualInformation()[1]), label=\"Layer 2\")\n",
    "plt.scatter(moving_average(net.layer3_IP.getMutualInformation()[0]), moving_average(net.layer3_IP.getMutualInformation()[1]), label=\"Layer 3\")\n",
    "plt.scatter(moving_average(net.layer4_IP.getMutualInformation()[0]), moving_average(net.layer4_IP.getMutualInformation()[1]), label=\"Layer 4\")\n",
    "# plt.scatter(moving_average(net.fc_IP.getMutualInformation()[0]), moving_average(net.fc_IP.getMutualInformation()[1]), label=\"Layer 5\")\n",
    "plt.scatter(moving_average(net.layer5_IP.getMutualInformation()[0]), moving_average(net.layer5_IP.getMutualInformation()[1]), label=\"Layer 5\")\n",
    "plt.xlabel(\"I(X, T)\")\n",
    "plt.ylabel(\"I(T, Y)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.layer1_IP.getMutualInformation()[0]\n",
    "net.layer1_IP.getMutualInformation()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn'):\n",
    "    ips = net.getInformationPlaneLayers()\n",
    "    # ips = [net.layer1_IP, net.layer2_IP, net.layer3_IP, net.layer4_IP, net.layer5_IP]\n",
    "    # ips = [net.layer1_IP, net.layer2_IP, net.layer3_IP, net.layer4_IP, net.fc_IP]\n",
    "    colors = ['Reds', 'Blues', 'binary', 'Greens', 'Oranges']\n",
    "\n",
    "    fig = plt.figure(constrained_layout=False)\n",
    "    gs1 = fig.add_gridspec(nrows=1, ncols=1, left=0.05, right=0.84, wspace=0.05)\n",
    "    gs2 = fig.add_gridspec(nrows=1, ncols=5, left=0.85, right=0.95, wspace=0)\n",
    "    f8_ax1 = fig.add_subplot(gs1[:, :])\n",
    "    f8_ax1.set_xlabel(\"I(X, T)\")\n",
    "    f8_ax1.set_ylabel(\"I(T, Y)\")\n",
    "\n",
    "    for idx, ip in enumerate(ips):\n",
    "        cmap = plt.cm.get_cmap(colors[idx])\n",
    "        Ixt, Ity = ip.getMutualInformation()\n",
    "        Ixt = moving_average(Ixt)\n",
    "        Ity = moving_average(Ity)\n",
    "        iterations = np.arange(len(Ixt))\n",
    "        color = np.array([cmap(iterations[-1])])\n",
    "        sc = f8_ax1.scatter(Ixt, Ity, c=iterations, vmin=0, vmax=iterations.max(), cmap=cmap, edgecolor=color)\n",
    "        f8_ax1.scatter([], [], c=color, label=\"Layer {}\".format(idx))\n",
    "\n",
    "        f8_ax2 = fig.add_subplot(gs2[0, idx])\n",
    "        cb = fig.colorbar(sc, cax=f8_ax2, pad=0)\n",
    "        cb.set_ticks([])\n",
    "\n",
    "    f8_ax1.legend()\n",
    "    cb.set_ticks([0, iterations.max()])\n",
    "    cb.set_label(\"Iterations\", labelpad=-18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "ips = net.getInformationPlaneLayers()\n",
    "\n",
    "Ixt_0, Ity_0 = ips[0].getMutualInformation()\n",
    "Ixt_1, Ity_1 = ips[-1].getMutualInformation()\n",
    "\n",
    "# print(Ixt_0)\n",
    "# print(Ity_0)\n",
    "# print(\"---\"*10)\n",
    "# print(Ixt_1)\n",
    "# print(Ity_1)\n",
    "labels_oh = one_hot(labels, num_classes=10).float()\n",
    "labels_test = labels.reshape((len(labels), 1)).float()\n",
    "# print(labels_test[0:100:10])\n",
    "# print(torch.cdist(labels_test[0:100:10], labels_test[0:100:10]))\n",
    "test = torch.cdist(labels_oh, labels_oh)\n",
    "print(torch.unique(test))\n",
    "test = torch.cdist(labels_test, labels_test)\n",
    "print(torch.unique(test))\n",
    "\n",
    "# print(labels_oh[0:100:10])\n",
    "# print(torch.cdist(labels_oh[0:100:10], labels_oh[0:100:10]))\n",
    "# print(torch.cdist(labels_oh, labels_oh))\n",
    "kernel = TensorKernel.RBF(labels_test, 0.25)\n",
    "# print(kernel)\n",
    "print(torch.unique(kernel))\n",
    "\n",
    "A = TensorKernel.RBF(labels_test[0:50:10], 0.1) / len(labels_test)\n",
    "print(A)\n",
    "# softmax = nn.Softmax()\n",
    "# output = net(inputs, labels)\n",
    "\n",
    "\n",
    "# test = InformationPlane(beta=0.5)\n",
    "\n",
    "# outputs = net(inputs, inputs, one_hot(labels, num_classes=10).float())\n",
    "# outputs = net.softmax(outputs)\n",
    "# sigma = net.fc_IP.sigma_optimizer.getSigma()\n",
    "\n",
    "# label = one_hot(labels, num_classes=10).float()\n",
    "# # print(label)\n",
    "# label_kernel_matrix = TensorKernel.RBF(label, 0.1)\n",
    "# print(label_kernel_matrix)\n",
    "# A = MatrixBasedRenyisEntropy.tensorRBFMatrix(outputs.flatten(1), sigma+3).detach()\n",
    "# # print(A)\n",
    "# Ay = MatrixBasedRenyisEntropy.tensorRBFMatrix(label, 0.1).detach()\n",
    "# # print(Ay)\n",
    "# Ax = MatrixBasedRenyisEntropy.tensorRBFMatrix(inputs.flatten(1), 8).detach()\n",
    "# # print(Ax)\n",
    "# # print(entropy(A))\n",
    "# # print(MatrixBasedRenyisEntropy.mutualInformation(Ax, A))\n",
    "\n",
    "# from scipy import linalg\n",
    "\n",
    "# def entropy(A : np.array):\n",
    "#         epsilon = 1e-6\n",
    "#         w = linalg.eigh(A, eigvals_only=True)\n",
    "#         # w, _ = LA.eig(A)\n",
    "#         w += epsilon\n",
    "#         print(w)\n",
    "#         return -np.sum(w * np.log2(w))\n",
    "\n",
    "# def testMutualInformation(Ax : np.array, Ay : np.array):\n",
    "#         entropy_Ax = MatrixBasedRenyisEntropy.entropy(Ax)\n",
    "#         entropy_Ay = entropy(Ay)\n",
    "#         # print(entropy_Ax)\n",
    "#         # print(entropy_Ay)\n",
    "#         joint_entropy_AxAy = MatrixBasedRenyisEntropy.jointEntropy(Ax, Ay)\n",
    "#         return (entropy_Ax + entropy_Ay - joint_entropy_AxAy)\n",
    "\n",
    "# print(testMutualInformation(A, Ay))\n",
    "# print(Ixt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def RBF(x, sigma):\n",
    "    distance = torch.pdist(x, x)\n",
    "    return torch.exp(-(distance**2)/(2*(sigma**2))).cpu()\n",
    "\n",
    "labels_oh = one_hot(labels, num_classes=10).float()\n",
    "labels_test = labels.reshape((len(labels), 1)).float()\n",
    "\n",
    "A = RBF(labels_test[5:10], 0.1) / len(labels_test[5:10])\n",
    "print(labels_test[5:10])\n",
    "print(A)\n",
    "# w, _ = LA.eig(A)\n",
    "# print(w)\n",
    "w = linalg.eigh(A, eigvals_only=True)\n",
    "print(w)\n",
    "# print(len(w))\n",
    "# print(np.log2(w))\n",
    "# print(np.log2(w)*w)\n",
    "# print(-np.sum(w * np.log2(w)))\n",
    "\n",
    "# w += 1e-16\n",
    "# print(w)\n",
    "# print(-np.sum(w * np.log2(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(labels, 'tests/labels.pt')\n",
    "# torch.save(inputs, 'tests/inputs.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, ip in enumerate(ips):\n",
    "    plt.plot(ip.sigma_optimizer.getSigmaValues(), label=\"layer {}\".format(idx+1))\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((3,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.cdist(a,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc81a3ec444beb1d5a523daf231afa571e79be8a57abb6fe0028623a3d4d7136"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('DeepLearning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
